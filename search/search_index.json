{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitask model that can perform multilingual speech recognition as well as speech translation and language identification.</p> <p>Join our Discord Community!</p> <p>\ud83c\udf89 Connect with other users, get help, and stay updated on the latest features! Join our Discord Server</p>"},{"location":"#features","title":"Features","text":"<p>Current release (v1.9.1) supports following whisper models:</p> <ul> <li>openai/whisper@v20250625</li> <li>SYSTRAN/faster-whisper@v1.1.1</li> <li>whisperX@v3.4.2</li> </ul>"},{"location":"#quick-usage","title":"Quick Usage","text":"<code>CPU</code> <code>GPU</code> <pre><code>docker run -d -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <pre><code>docker run -d --gpus all -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest-gpu\n</code></pre> <p>for more information:</p> <ul> <li>Documentation/Run</li> <li>Docker Hub</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>This software uses libraries from the FFmpeg project under the LGPLv2.1</li> </ul>"},{"location":"build/","title":"Development","text":""},{"location":"build/#development-environment","title":"Development Environment","text":"<p>Install poetry v2.X with following command:</p> <pre><code>pip3 install poetry\n</code></pre>"},{"location":"build/#installation","title":"Installation","text":"<p>Install dependencies for cpu</p> <pre><code>poetry install --extras cpu\n</code></pre> <p>Install dependencies for cuda</p> <pre><code>poetry install --extras cuda\n</code></pre> <p>Note</p> <p>By default, this will install the CPU version of PyTorch. For GPU support, you'll need to install the appropriate CUDA version of PyTorch separately: <pre><code># For CUDA support (example for CUDA 11.8):\npip3 install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu121\n</code></pre></p>"},{"location":"build/#run","title":"Run","text":"<p>Starting the Webservice:</p> <pre><code>poetry run whisper-asr-webservice --host 0.0.0.0 --port 9000\n</code></pre>"},{"location":"build/#build","title":"Build","text":"<code>Docker</code> <code>Poetry</code> <p>With <code>Dockerfile</code>:</p> <code>CPU</code> <code>GPU</code> <pre><code># Build Image\ndocker build -t whisper-asr-webservice .\n\n# Run Container\ndocker run -d -p 9000:9000 whisper-asr-webservice\n# or with specific model\ndocker run -d -p 9000:9000 -e ASR_MODEL=base whisper-asr-webservice\n</code></pre> <pre><code># Build Image\ndocker build -f Dockerfile.gpu -t whisper-asr-webservice-gpu .\n\n# Run Container\ndocker run -d --gpus all -p 9000:9000 whisper-asr-webservice-gpu\n# or with specific model\ndocker run -d --gpus all -p 9000:9000 -e ASR_MODEL=base whisper-asr-webservice-gpu\n</code></pre> <p>With <code>docker-compose</code>:</p> <code>CPU</code> <code>GPU</code> <pre><code>docker-compose up --build\n</code></pre> <pre><code>docker-compose -f docker-compose.gpu.yml up --build\n</code></pre> <p>Build .whl package</p> <pre><code>poetry build\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#191-2025-07-01","title":"1.9.1 (2025-07-01)","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed Whisperx diarization pipeline initialization</li> <li>Fixed Whisperx language detection</li> </ul>"},{"location":"changelog/#190-2025-06-29","title":"1.9.0 (2025-06-29)","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Upgraded</li> <li>Poetry to v2.1.3</li> <li>openai/whisper@v20250625</li> <li>SYSTRAN/faster-whisper to v1.1.1</li> <li>whisperX@v3.4.2</li> <li>torch to v2.7.1</li> <li>torchaudio to v2.7.1</li> <li>numpy to v2.2.6</li> <li>fastapi to v0.115.14</li> <li>uvicorn to v0.35.0</li> <li>numba to v0.61.2</li> </ul>"},{"location":"changelog/#182-2025-02-18","title":"1.8.2 (2025-02-18)","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Reduced GPU image size by using <code>nvidia/cuda:12.6.3-base-ubuntu22.04</code></li> </ul>"},{"location":"changelog/#181-2025-02-18","title":"1.8.1 (2025-02-18)","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed issues with Torch CUDA and cuDNN</li> <li>Updated Torch and Torchaudio dependencies for multi-architecture support</li> </ul>"},{"location":"changelog/#180-2025-02-17","title":"1.8.0 (2025-02-17)","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added support for whisperX@v3.1.1</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Upgraded Cuda GPU image to v12.6.3</li> <li>Upgraded dependencies</li> <li>torch to v2.6.0</li> <li>fastapi to v0.115.8</li> <li>llvmlite to v0.44.0</li> <li>numba to v0.61.0</li> <li>ruff to v0.9.6</li> <li>black to v25.1.0</li> <li>mkdocs-material to v9.6.4</li> <li>pymdown-extensions to v10.14.3</li> </ul>"},{"location":"changelog/#171-2024-12-18","title":"1.7.1 (2024-12-18)","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fix JSON serialization of segments due to Faster Whisper v1.1.0 changes</li> </ul>"},{"location":"changelog/#170-2024-12-17","title":"1.7.0 (2024-12-17)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Timeout configured to allow model to be unloaded when idle</li> <li>Added detection confidence to langauge detection endpoint</li> <li>Set mel generation to adjust n_dims automatically to match the loaded model</li> <li>Refactor classes, Add comments, implement abstract methods, and add factory method for engine selection</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Upgraded</li> <li>SYSTRAN/faster-whisper to v1.1.0</li> <li>uvicorn to v0.34.0</li> <li>tqdm to v4.67.1</li> <li>python-multipart to v0.0.20</li> <li>fastapi to v0.115.6</li> <li>pytest to v8.3.4</li> <li>ruff to v0.8.3</li> <li>black to v24.10.0</li> <li>mkdocs to v1.6.1</li> <li>mkdocs-material to v9.5.49</li> <li>pymdown-extensions to v10.12</li> </ul>"},{"location":"changelog/#160-2024-10-06","title":"1.6.0 (2024-10-06)","text":""},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Upgraded</li> <li>openai/whisper@v20240930</li> <li>fastapi to v0.115.0</li> <li>uvicorn to v0.31.0</li> <li>tqdm to v4.66.5</li> <li>python-multipart to v0.0.12</li> </ul>"},{"location":"changelog/#150-2024-07-04","title":"1.5.0 (2024-07-04)","text":""},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Upgraded</li> <li>SYSTRAN/faster-whisper to v1.0.3</li> <li>fastapi to v0.111.0</li> <li>uvicorn to v0.30.1</li> <li>gunicorn to v22.0.0</li> <li>tqdm to v4.66.4</li> <li>llvmlite to v0.43.0</li> <li>numba to v0.60.0</li> </ul>"},{"location":"changelog/#141-2024-04-17","title":"1.4.1 (2024-04-17)","text":""},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Upgraded torch to v1.13.1</li> </ul>"},{"location":"changelog/#140-2024-04-17","title":"1.4.0 (2024-04-17)","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Upgraded</li> <li>SYSTRAN/faster-whisper to v1.0.1</li> <li>fastapi to v0.110.1</li> <li>uvicorn to v0.29.0</li> <li>gunicorn to v21.2.0</li> <li>tqdm to v4.66.2</li> <li>python-multipart to v0.0.9</li> <li>llvmlite to v0.42.0</li> <li>numba to v0.59.1</li> </ul>"},{"location":"changelog/#130-2024-02-15","title":"1.3.0 (2024-02-15)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Compiled and added FFmpeg without LGPL libraries for license compliance</li> </ul>"},{"location":"changelog/#124-2023-11-27","title":"1.2.4 (2023-11-27)","text":""},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Upgraded</li> <li>openai/whisper to v20231117</li> <li>SYSTRAN/faster-whisper to v0.10.0</li> </ul>"},{"location":"changelog/#123-2023-11-07","title":"1.2.3 (2023-11-07)","text":""},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Upgraded</li> <li>openai/whisper to v20231106</li> </ul>"},{"location":"changelog/#122-2023-11-03","title":"1.2.2 (2023-11-03)","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed <code>swagger-ui</code> rendering issues by upgrading to <code>v5.9.1</code>, fixes #153 and #154</li> </ul>"},{"location":"changelog/#121-2023-11-03","title":"1.2.1 (2023-11-03)","text":""},{"location":"changelog/#enabled","title":"Enabled","text":"<ul> <li>Enabled <code>vad_filter</code> for <code>faster-whisper</code> engine</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Changed misspelling in \"Word level timestamps\"</li> <li>Removed unused unidecode dependency</li> <li>Upgraded</li> <li>uvicorn to v0.23.2</li> <li>gunicorn to v21.0.1</li> <li>tqdm to v4.66.1</li> <li>python-multipart to v0.0.6</li> <li>fastapi to v0.104.1</li> <li>llvmlite to v0.41.1</li> <li>numba to v0.58.0</li> </ul>"},{"location":"changelog/#120-2023-10-01","title":"1.2.0 (2023-10-01)","text":""},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>Upgraded</li> <li>openai/whisper to v20230918</li> <li>guillaumekln/faster-whisper to v0.9.0</li> </ul>"},{"location":"changelog/#updated","title":"Updated","text":"<ul> <li>Updated model conversion method (for Faster Whisper) to use Hugging Face downloader</li> <li>Updated default model paths to <code>~/.cache/whisper</code> or <code>/root/.cache/whisper</code>.</li> <li>For customization, modify the <code>ASR_MODEL_PATH</code> environment variable.</li> <li> <p>Ensure Docker volume is set for the corresponding directory to use caching.</p> <pre><code>docker run -d -p 9000:9000 -e ASR_MODEL_PATH=/data/whisper -v $PWD/yourlocaldir:/data/whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> </li> <li> <p>Removed the <code>triton</code> dependency from <code>poetry.lock</code> to ensure the stability of the pipeline for <code>ARM-based</code> Docker images</p> </li> </ul>"},{"location":"changelog/#111-2023-05-29","title":"1.1.1 (2023-05-29)","text":""},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>94 gpus that don't support float16 in #103</li> <li>Update compute type in #108</li> <li>Add word level functionality for Faster Whisper in #109</li> </ul>"},{"location":"changelog/#110-2023-04-17","title":"1.1.0 (2023-04-17)","text":""},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Docs in #72</li> <li>Fix language code typo in #77</li> <li>Adds support for FasterWhisper in #81</li> <li>Add an optional param to skip the encoding step in #82</li> <li>Faster whisper in #92</li> </ul>"},{"location":"changelog/#106-2023-02-05","title":"1.0.6 (2023-02-05)","text":""},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>Update README.md in #58</li> <li>68 update the versions in #69</li> <li>Fix gunicorn run command and remove deprecated poetry run script in #70</li> <li>Move torch installation method into the pyproject.toml file in #71</li> <li>Add prompt to ASR in #66</li> </ul>"},{"location":"changelog/#105-2022-12-08","title":"1.0.5 (2022-12-08)","text":""},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>43 make swagger doc not depend on internet connection in #52</li> <li>Add new large model v2 in #53</li> </ul>"},{"location":"changelog/#104-2022-11-28","title":"1.0.4 (2022-11-28)","text":""},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li>43 make swagger doc not depend on internet connection in #51</li> <li>Anally retentively fixed markdown linting warnings in README. Sorry. in #48</li> <li>Explicit macOS readme with explanation for no-GPU [closes #44] in #47</li> </ul>"},{"location":"changelog/#103-beta-2022-11-17","title":"1.0.3-beta (2022-11-17)","text":""},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>Combine transcribe endpoints in #36</li> <li>Add multi worker support with gunicorn in #37</li> <li>Add multi platform (amd &amp; arm) support in #39</li> <li>Upgrade Cuda version to 11.7 in #40</li> <li>Lock to the latest whisper version (eff383) in #41</li> </ul>"},{"location":"changelog/#102-beta-2022-10-04","title":"1.0.2-beta (2022-10-04)","text":""},{"location":"changelog/#changed_18","title":"Changed","text":"<ul> <li>add mutex lock to the model in #19</li> <li>Subtitles in #21</li> <li>Add gpu support and create Docker image for cuda with GitHub flow in #22</li> </ul>"},{"location":"changelog/#101-beta-2022-09-27","title":"1.0.1-beta (2022-09-27)","text":""},{"location":"changelog/#changed_19","title":"Changed","text":"<ul> <li>Init GitHub runners in #10</li> <li>Lock Whisper dependency with b4308... revision number to prevent build crashes in #15</li> </ul>"},{"location":"changelog/#100-beta-2022-09-25","title":"1.0.0-beta (2022-09-25)","text":""},{"location":"changelog/#changed_20","title":"Changed","text":"<ul> <li>Docker init in #1</li> <li>Create LICENCE in #2</li> <li>Fastapi init in #3</li> <li>Avoid temp file in #4</li> <li>Translate init in #5</li> <li>mp3 support by using FFmpeg instead of librosa in #8</li> <li>add language detection endpoint in #9</li> </ul>"},{"location":"endpoints/","title":"API Endpoints","text":""},{"location":"endpoints/#quick-start","title":"Quick start","text":"<p>After running the docker image interactive Swagger API documentation is available at localhost:9000/docs</p> <p>There are 2 endpoints available:</p> <ul> <li>/asr (Automatic Speech Recognition)</li> <li>/detect-language</li> </ul>"},{"location":"endpoints/#automatic-speech-recognition-service-asr","title":"Automatic speech recognition service /asr","text":"<ul> <li>2 task choices:</li> <li>transcribe: (default) task, transcribes the uploaded file.</li> <li>translate: will provide an English transcript no matter which language was spoken.</li> <li>Files are automatically converted with FFmpeg.</li> <li>Full list of supported audio and video formats.</li> <li>You can enable word level timestamps output by <code>word_timestamps</code> parameter</li> <li>You can Enable the voice activity detection (VAD) to filter out parts of the audio without speech  by <code>vad_filter</code> parameter (only with <code>Faster Whisper</code> for now).</li> </ul>"},{"location":"endpoints/#request-url-query-params","title":"Request URL Query Params","text":"Name Values Description audio_file File Audio or video file to transcribe output <code>text</code> (default), <code>json</code>, <code>vtt</code>, <code>srt</code>, <code>tsv</code> Output format task <code>transcribe</code>, <code>translate</code> Task type - transcribe in source language or translate to English language <code>en</code> (default is auto recognition) Source language code (see supported languages) word_timestamps false (default) Enable word-level timestamps (Faster Whisper only) vad_filter false (default) Enable voice activity detection filtering (Faster Whisper only) encode true (default) Encode audio through FFmpeg before processing diarize false (default) Enable speaker diarization (WhisperX only) min_speakers null (default) Minimum number of speakers for diarization (WhisperX only) max_speakers null (default) Maximum number of speakers for diarization (WhisperX only) <p>Example request with cURL</p> <pre><code>curl -X POST -H \"content-type: multipart/form-data\" -F \"audio_file=@/path/to/file\" 0.0.0.0:9000/asr?output=json\n</code></pre>"},{"location":"endpoints/#response-json","title":"Response (JSON)","text":"<ul> <li>text: Contains the full transcript</li> <li>segments: Contains an entry per segment. Each entry provides <code>timestamps</code>, <code>transcript</code>, <code>token ids</code>, <code>word level timestamps</code> and other metadata</li> <li>language: Detected or provided language (as a language code)</li> </ul>"},{"location":"endpoints/#response-formats","title":"Response Formats","text":"<p>The API supports multiple output formats:</p> <ul> <li>text: Plain text transcript (default)</li> <li>json: Detailed JSON with segments, timestamps, and metadata</li> <li>vtt: WebVTT subtitle format</li> <li>srt: SubRip subtitle format  </li> <li>tsv: Tab-separated values with timestamps</li> </ul>"},{"location":"endpoints/#supported-languages","title":"Supported Languages","text":"<p>The service supports all languages supported by Whisper. Some common language codes:</p> <ul> <li>Turkish (tr)</li> <li>English (en)</li> <li>Spanish (es)</li> <li>French (fr)</li> <li>German (de)</li> <li>Italian (it)</li> <li>Portuguese (pt)</li> <li>And many more...</li> </ul> <p>See the Whisper documentation for the full list of supported languages.</p>"},{"location":"endpoints/#speaker-diarization","title":"Speaker Diarization","text":"<p>When using the WhisperX engine with diarization enabled (<code>diarize=true</code>), the output will include speaker labels for each segment. This requires:</p> <ol> <li>WhisperX engine to be configured</li> <li>Valid Hugging Face token set in HF_TOKEN</li> <li>Sufficient memory for diarization models</li> </ol> <p>You can optionally specify <code>min_speakers</code> and <code>max_speakers</code> if you know the expected number of speakers.</p>"},{"location":"endpoints/#language-detection-service-detect-language","title":"Language detection service /detect-language","text":"<p>Detects the language spoken in the uploaded file. Only processes first 30 seconds.</p> <p>Returns a json with following fields:</p> <ul> <li>detected_language: Human readable language name (e.g. \"english\")</li> <li>language_code: ISO language code (e.g. \"en\")</li> <li>confidence: Confidence score between 0 and 1 indicating detection reliability</li> </ul> <p>Example response:</p> <pre><code>{\n    \"detected_language\": \"english\",\n    \"language_code\": \"en\",\n    \"confidence\": 0.98\n}\n</code></pre>"},{"location":"environmental-variables/","title":"Configuration","text":""},{"location":"environmental-variables/#configuring-the-engine","title":"Configuring the <code>Engine</code>","text":"<code>openai_whisper</code> <code>faster_whisper</code> <code>whisperx</code> <pre><code>export ASR_ENGINE=openai_whisper\n</code></pre> <pre><code>export ASR_ENGINE=faster_whisper\n</code></pre> <pre><code>export ASR_ENGINE=whisperx\n</code></pre>"},{"location":"environmental-variables/#configuring-the-model","title":"Configuring the <code>Model</code>","text":"<pre><code>export ASR_MODEL=base\n</code></pre> <p>Available ASR_MODELs are:</p> <ul> <li>Standard models: <code>tiny</code>, <code>base</code>, <code>small</code>, <code>medium</code>, <code>large-v1</code>, <code>large-v2</code>, <code>large-v3</code> (or <code>large</code>), <code>large-v3-turbo</code> (or <code>turbo</code>)</li> <li>English-optimized models: <code>tiny.en</code>, <code>base.en</code>, <code>small.en</code>, <code>medium.en</code></li> <li>Distilled models: <code>distil-large-v2</code>, <code>distil-medium.en</code>, <code>distil-small.en</code>, <code>distil-large-v3</code> (only for whisperx and faster-whisper)</li> </ul> <p>For English-only applications, the <code>.en</code> models tend to perform better, especially for the <code>tiny.en</code> and <code>base.en</code> models. We observed that the difference becomes less significant for the <code>small.en</code> and <code>medium.en</code> models.</p> <p>The distilled models offer improved inference speed while maintaining good accuracy.</p>"},{"location":"environmental-variables/#configuring-the-model-path","title":"Configuring the <code>Model Path</code>","text":"<pre><code>export ASR_MODEL_PATH=/data/whisper\n</code></pre>"},{"location":"environmental-variables/#configuring-the-model-unloading-timeout","title":"Configuring the <code>Model Unloading Timeout</code>","text":"<pre><code>export MODEL_IDLE_TIMEOUT=300\n</code></pre> <p>Defaults to <code>0</code>. After no activity for this period (in seconds), unload the model until it is requested again. Setting <code>0</code> disables the timeout, keeping the model loaded indefinitely.</p>"},{"location":"environmental-variables/#configuring-the-sample_rate","title":"Configuring the <code>SAMPLE_RATE</code>","text":"<pre><code>export SAMPLE_RATE=16000\n</code></pre> <p>Defaults to <code>16000</code>. Default sample rate for audio input. <code>16 kHz</code> is commonly used in <code>speech-to-text</code> tasks.</p>"},{"location":"environmental-variables/#configuring-device-and-quantization","title":"Configuring Device and Quantization","text":"<pre><code>export ASR_DEVICE=cuda  # or 'cpu'\nexport ASR_QUANTIZATION=float32  # or 'float16', 'int8'\n</code></pre> <p>The <code>ASR_DEVICE</code> defaults to <code>cuda</code> if GPU is available, otherwise <code>cpu</code>. </p> <p>The <code>ASR_QUANTIZATION</code> defines the precision for model weights:</p> <ul> <li><code>float32</code>: 32-bit floating-point precision (higher precision, slower inference)</li> <li><code>float16</code>: 16-bit floating-point precision (lower precision, faster inference)</li> <li><code>int8</code>: 8-bit integer precision (lowest precision, fastest inference)</li> </ul> <p>Defaults to <code>float32</code> for GPU, <code>int8</code> for CPU.</p>"},{"location":"environmental-variables/#configuring-subtitle-options-whisperx","title":"Configuring Subtitle Options (WhisperX)","text":"<pre><code>export SUBTITLE_MAX_LINE_WIDTH=1000\nexport SUBTITLE_MAX_LINE_COUNT=2\nexport SUBTITLE_HIGHLIGHT_WORDS=false\n</code></pre> <p>These options only apply when using the WhisperX engine:</p> <ul> <li><code>SUBTITLE_MAX_LINE_WIDTH</code>: Maximum width of subtitle lines (default: 1000)</li> <li><code>SUBTITLE_MAX_LINE_COUNT</code>: Maximum number of lines per subtitle (default: 2)</li> <li><code>SUBTITLE_HIGHLIGHT_WORDS</code>: Enable word highlighting in subtitles (default: false)</li> </ul>"},{"location":"environmental-variables/#hugging-face-token","title":"Hugging Face Token","text":"<pre><code>export HF_TOKEN=your_token_here\n</code></pre> <p>Required when using the WhisperX engine to download the diarization model.</p>"},{"location":"licence/","title":"Licence","text":"<pre><code>MIT License\n\nCopyright (c) 2022 Ahmet Oner &amp; Besim Alibegovic\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"run/","title":"Installation & Usage","text":""},{"location":"run/#usage","title":"Usage","text":"<p>Whisper ASR Webservice now available on Docker Hub. You can find the latest version of this repository on docker hub for CPU and GPU.</p> <p>Docker Hub: https://hub.docker.com/r/onerahmet/openai-whisper-asr-webservice</p> <code>CPU</code> <code>CPU (macOS)</code> <code>GPU</code> <pre><code>docker pull onerahmet/openai-whisper-asr-webservice:latest\ndocker run -d -p 9000:9000 \\\n  -e ASR_MODEL=base \\\n  -e ASR_ENGINE=openai_whisper \\\n  onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <p>GPU passthrough does not work on macOS due to fundamental design limitations of Docker. Docker actually runs containers within a LinuxVM on macOS. If you wish to run GPU-accelerated containers, I'm afraid Linux is your only option.</p> <p>The <code>:latest</code> image tag provides both amd64 and arm64 architectures:</p> <pre><code>docker pull onerahmet/openai-whisper-asr-webservice:latest\ndocker run -d -p 9000:9000 \\\n  -e ASR_MODEL=base \\\n  -e ASR_ENGINE=openai_whisper \\\n  onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <pre><code>docker pull onerahmet/openai-whisper-asr-webservice:latest-gpu\ndocker run -d --gpus all -p 9000:9000 \\\n  -e ASR_MODEL=base \\\n  -e ASR_ENGINE=openai_whisper \\\n  onerahmet/openai-whisper-asr-webservice:latest-gpu\n</code></pre>"},{"location":"run/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables can be used to configure the service:</p> <ul> <li><code>ASR_MODEL</code>: Whisper model to use (tiny, base, small, medium, large) [default: base]</li> <li><code>ASR_ENGINE</code>: ASR engine to use (openai_whisper, faster_whisper) [default: openai_whisper]</li> <li><code>ASR_MODEL_PATH</code>: Custom path to store/load model files [optional]</li> </ul> <p>Interactive Swagger API documentation is available at http://localhost:9000/docs</p> <p></p>"},{"location":"run/#cache","title":"Cache","text":"<p>The ASR model is downloaded each time you start the container. Using the large model can take significant time to download. To reduce container startup time by avoiding repeated downloads, you can persist the cache directory to local storage. The model will then be loaded from the cache instead of being downloaded again on subsequent container starts.</p> <p>Important: Using a persistent cache will prevent you from receiving model updates.</p> <code>Default cache dir</code> <code>With ASR_MODEL_PATH</code> <pre><code>docker run -d -p 9000:9000 \\\n  -v $PWD/cache:/root/.cache \\\n  onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <pre><code>docker run -d -p 9000:9000 \\\n  -e ASR_MODEL_PATH=/data/whisper \\\n  -v $PWD/cache:/data/whisper \\\n  onerahmet/openai-whisper-asr-webservice:latest\n</code></pre>"}]}